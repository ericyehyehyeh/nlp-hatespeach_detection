{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from fuzzywuzzy import fuzz\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_dis(distance):\n",
    "    if (1 - (distance -1)* 0.1 > 0):\n",
    "        return 1 - (distance -1)* 0.1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def nlp_API(text, client):\n",
    "    text = text\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "\n",
    "    return sentiment.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = language.LanguageServiceClient()\n",
    "all_races = np.load(\"all_races.npy\", encoding=\"bytes\")\n",
    "fword = list(np.load(\"fword.npy\",encoding=\"bytes\"))\n",
    "gender = ['men','woman','lesbian','gay','bisexual','transgender','Trans','queer','questioning','intersex']\n",
    "# below are dict \n",
    "hate_All = np.load(\"hate_All.npy\", encoding=\"bytes\").item()\n",
    "racial = np.load(\"racial.npy\", encoding=\"bytes\").item()\n",
    "# this is a list of keys of hate_All\n",
    "hate_keys = list(hate_All.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"data.txt\", 'r')\n",
    "sentences = []\n",
    "label_true = []\n",
    "count = 0\n",
    "for line in file:\n",
    "    if line.split(\"-\")[0] == '3':\n",
    "        label_true.append(1)\n",
    "    else:\n",
    "        label_true.append(0)\n",
    "    sentences.append(line.split(\"-\")[1].split())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0.85 0 0\n",
      "1 0 0.85 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.85 0 0\n",
      "1 0 0.85 0 0.0\n",
      "1 1.0 0.85 0 0.30000001192092896\n",
      "1 0 0.85 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.85 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.85 0 0.0\n",
      "1 0 0.85 0 0\n",
      "1 0 0.85 0 0.0\n",
      "1 0 0.85 0 0\n",
      "0 0 0 0 0\n",
      "3 0.8 2.18 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.85 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.85 0 0.30000001192092896\n",
      "0 0 0 0 0\n",
      "1 0 0.85 0 0.0\n",
      "1 0 0.85 0 0.5\n",
      "1 0 0.85 0 0.0\n",
      "1 0 0.85 0 0.0\n",
      "1 0.9 0.85 0 0.20000000298023224\n",
      "1 0 0.85 0 0\n",
      "1 1.0 0.85 0 0.4000000059604645\n",
      "1 0 0.85 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.85 0 0.0\n",
      "1 0 0.85 0 0.0\n",
      "0 0 0 0 0\n",
      "2 0.3999999999999999 0 0 0.800000011920929\n",
      "1 0 0.88 0 0.0\n",
      "1 0 0.88 0 0.0\n",
      "1 0 0.88 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.88 0 0.10000000149011612\n",
      "1 0 0.88 0 0.6000000238418579\n",
      "1 0.9 0.88 0 0.30000001192092896\n",
      "1 0 0.88 0 0.10000000149011612\n",
      "1 0 0.88 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0.8 0.88 0 0.0\n",
      "1 0 0.88 0 0.0\n",
      "1 0 0.88 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.88 0 0.0\n",
      "1 0 0.88 0 0\n",
      "1 1.0 0.88 0 0.20000000298023224\n",
      "1 0 0.88 0 0.0\n",
      "1 0 0.88 0 0\n",
      "1 0 0.88 0 0.0\n",
      "1 0 0.88 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.88 0 0.0\n",
      "0 0 0 0 0\n",
      "2 1.0 0.88 0 0.0\n",
      "1 0.9 0.88 0 0.4000000059604645\n",
      "1 0 0.88 0 0\n",
      "3 0.8 1.76 0 0.20000000298023224\n",
      "1 1.0 0.88 0 0.30000001192092896\n",
      "1 0 0.88 0 0\n",
      "1 0 0.88 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.88 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.56 0 0\n",
      "1 0.7 0.56 0 0.699999988079071\n",
      "1 0.3999999999999999 0.56 0 0.30000001192092896\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.56 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 1.0 0.56 0 0.699999988079071\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0.7 0.56 0 0.0\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0\n",
      "1 0 0.56 0 0\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0.0\n",
      "1 1.0 0.56 0 0.4000000059604645\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.56 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.56 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0.8 0.56 0 0.0\n",
      "1 0.7 0.56 0 0\n",
      "1 1.0 0.56 0 0.800000011920929\n",
      "1 0 0.56 0 0.20000000298023224\n",
      "1 0 0.56 0 0\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0\n",
      "1 0 0.56 0 0.4000000059604645\n",
      "1 0.9 0.56 0 0.4000000059604645\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0\n",
      "1 0 0.56 0 0\n",
      "1 0.9 0.56 0 0\n",
      "1 0.19999999999999996 0.56 0 0.0\n",
      "1 0 0.56 0 0.0\n",
      "1 0 0.56 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.6000000238418579\n",
      "1 0.8 0.49 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.49 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0.5 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "2 0 0.49 0 0.20000000298023224\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.49 0 0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "1 0.5 0.49 0 0.0\n",
      "1 0.6 0.49 0 0.0\n",
      "1 0 0.49 0 0\n",
      "1 0 0.49 0 0.20000000298023224\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "1 0.3999999999999999 0.49 0 0\n",
      "1 0 0.0 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "1 0.7 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.49 0 0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.10000000149011612\n",
      "1 0 0.49 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.49 0 0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0\n",
      "1 0 0.49 0 0.0\n",
      "1 0 0.49 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 1.0 0.5 0 0.30000001192092896\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "2 0.7 0.94 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0.0\n",
      "1 1.0 0.5 0 0\n",
      "1 0 0.5 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0\n",
      "2 0.6 1.0 0 0\n",
      "2 0.6 1.0 0 0\n",
      "3 0.7 1.35 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0.0\n",
      "0 0 0 0 0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "1 0 0.5 0 0.0\n",
      "2 0.8 0.5 0 0.0\n",
      "1 1.0 0.5 0 0.0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n",
      "0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "file1 = open(\"feature.txt\", 'w')\n",
    "count = 1\n",
    "for sentence in sentences[14269:]:\n",
    "    #print(count)\n",
    "    sentence_changed = ' '.join(sentence)\n",
    "    count += 1\n",
    "    fword_index = []\n",
    "    special_word_index = []\n",
    "    feature_vec = []\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] in fword:\n",
    "            fword_index.append(i)\n",
    "        if sentence[i] in gender or sentence[i] in all_races or sentence[i] in racial.keys() or sentence[i] in hate_keys:\n",
    "            special_word_index.append(i)\n",
    "        # if no special word, return all zero vector\n",
    "    if len(special_word_index) == 0:\n",
    "        feature_vec = [0,0,0,0,0]\n",
    "        #print(feature_vec)\n",
    "        #break\n",
    "    else:\n",
    "        feature_vec.append(len(special_word_index))\n",
    "\n",
    "        # eturne 2 distance between special word and fword\n",
    "        min_dis = 100\n",
    "        for fword_i in fword_index:\n",
    "            for special_word_i in special_word_index:\n",
    "                if(fword_i != special_word_i):\n",
    "                    min_dis = min(min_dis, abs(fword_i - special_word_i))\n",
    "\n",
    "        feature_vec.append(convert_dis(min_dis))\n",
    "\n",
    "        # feture 3 offensiveness\n",
    "        offensive = 0\n",
    "        for special_word_i in special_word_index:\n",
    "            if sentence[special_word_i] in hate_keys:\n",
    "                offensive += float(hate_All[sentence[special_word_i]])\n",
    "        feature_vec.append(offensive)\n",
    "\n",
    "        # dict of racial and gender\n",
    "        exist = 0\n",
    "        for special_word_i in special_word_index:\n",
    "            temp = sentence[special_word_i]\n",
    "            if temp in racial.keys():\n",
    "                for racial_value in racial[temp]:\n",
    "                    racial_value = racial_value.lower()\n",
    "                    if fuzz.partial_ratio(racial_value, sentence_changed) >= 0.88:\n",
    "                        exist = 1\n",
    "                        break\n",
    "        feature_vec.append(exist)\n",
    "\n",
    "        #print(special_word_index)\n",
    "        # google api score\n",
    "        api_value = 0\n",
    "        for i in special_word_index:\n",
    "            if i - 2 >= 0:\n",
    "                left_bigram = sentence[i -2] + ' ' + sentence[i - 1] \n",
    "                try:\n",
    "                    api_value = min(nlp_API(left_bigram, client), api_value)\n",
    "                except:\n",
    "                    time.sleep(1)\n",
    "                    try:\n",
    "                        api_value = min(nlp_API(left_bigram, client), api_value)\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                        api_value = min(nlp_API(left_bigram, client), api_value)\n",
    "            elif i - 1 == 0:\n",
    "                left_bigram = sentence[i - 1] \n",
    "                try:\n",
    "                    api_value = min(nlp_API(left_bigram, client), api_value)\n",
    "                except:\n",
    "                    time.sleep(1)\n",
    "                    try:\n",
    "                        api_value = min(nlp_API(left_bigram, client), api_value)\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                        api_value = min(nlp_API(left_bigram, client), api_value)\n",
    "            if i + 2 < len(sentence):\n",
    "                right_bigram = sentence[i + 1] + ' ' + sentence[i + 2]\n",
    "                try:\n",
    "                    api_value = min(nlp_API(right_bigram, client), api_value)\n",
    "                except:\n",
    "                    time.sleep(1)\n",
    "                    try:\n",
    "                        api_value = min(nlp_API(right_bigram, client), api_value)\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                        api_value = min(nlp_API(right_bigram, client), api_value)\n",
    "            elif i + 1 < len(sentence):\n",
    "                right_bigram = sentence[i + 1]\n",
    "                try:\n",
    "                    api_value = min(nlp_API(right_bigram, client), api_value)\n",
    "                except:\n",
    "                    time.sleep(1)\n",
    "                    try:\n",
    "                        api_value = min(nlp_API(right_bigram, client), api_value)\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                        api_value = min(nlp_API(right_bigram, client), api_value)\n",
    "        if api_value > 0:\n",
    "            feature_vec.append(0)\n",
    "        else:\n",
    "            feature_vec.append(abs(api_value))\n",
    "\n",
    "    #print(feature_vec)\n",
    "    print(str(feature_vec[0])+ ' ' + str(feature_vec[1]) + ' ' + str(feature_vec[2]) + ' ' + str(feature_vec[3])+ ' ' + str(feature_vec[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'texi drver'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "racial_value = \"Texi drVer\"\n",
    "racial_value = racial_value.lower()\n",
    "racial_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(\"text driver\", \"indian stupid text driver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['warning:', 'penny', 'boards', 'will', 'make', 'you', 'a', 'faggot']\n",
      "['fuck', 'dyke']\n",
      "['at', 'least', 'i', 'dont', 'look', 'like', 'jefree', 'starr', 'faggot']\n",
      "['is', 'a', 'fag\"', 'jackie', 'jealous\"', 'neeeee']\n",
      "['you', 'heard', 'me', 'bitch', 'but', 'any', 'way', \"i'm\", 'back', 'th', 'texas', 'so', 'wtf', 'u', 'talking', 'about', 'bitch', 'ass', 'nigga']\n",
      "['your', 'a', 'dirty', 'terrorist', 'and', 'your', 'religion', 'is', 'a', 'fucking', 'joke,', 'you', 'go', 'around', 'screaming', 'allah', 'akbar', 'doing', 'terrorist', 'shit.', 'dirty', 'faggot.']\n",
      "['rt', 'looking', 'like', 'faggots?']\n",
      "['well', 'i', 'thought', 'you', 'knew', 'actually', 'rt', 'man', 'why', \"y'all\", \"didn't\", 'tell', 'me', 'i', 'was', 'a', 'dick', 'riding', 'ass', 'faggot?', \"y'all\", 'not', 'real']\n",
      "['i', 'know.', 'it', 'was', 'a', 'joke,', 'faggot.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences[0:9]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
