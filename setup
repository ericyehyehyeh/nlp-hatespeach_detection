SETUP:
A. Special Word:
a) hatebase word in hate_all
b) Racial word: set(racial_dict.keys() + nationality)
c) Gender

B. Five Features
1. #pairs of f_word & special word
2. min distance (#words) between f_word and special word
3. [optional]: offensive word count from hatebase hate_all
4. offensive score -- add up from hatebase
5. [optional]: #nationality word + #ethnicity word + #gender word count
6. offensive score defined by google api nlp: 2-gram before & after special word or hatebase word

C: GOOGLE NLP API
-- REMEMBER TO SET UP GOOGLE SERVICE ACCOUNT AND ENABLE IT:
      https://cloud.google.com/docs/authentication/getting-started

-- RUN THE FOLLOWING CODE, pip install google pip install google.cloud
      from google.cloud import language
      from google.cloud.language import enums
      from google.cloud.language import types
      client = language.LanguageServiceClient()
      text = u'fuck u sucks'
      document = types.Document(content=text,type=enums.Document.Type.PLAIN_TEXT)
      sentiment = client.analyze_sentiment(document=document).document_sentiment
      print('Sentiment: {}'.format(sentiment.score))
â€¢ IF FAIL, follow the link to enable the NLP-API

Things need to notice:
0. !!! if no special word and hatebase word found, classify this sentence as normal and continue

1. fuzz.partial_ratio("taxi driver", "......(sentences)......") to check if sentence contain text driver
## might want to check if taxi and driver are separate or together

2. ?? f_word and special word(country, ethnicity) might have multiple pair, and different sequence, need to decide how to 
   measure distance
   
3. ?? when using hatebase: find all hatebase word and MAX? AVG?

4. ?? default valued needed to be set: 
      a) what if no pair of f-word and special word found
      b) what if did not find hatebase word?
      c) what is found indian but did not find texi driver?
      d) !!! offensive score for 6 always exist: must exist special word or hatebase word, then do not consider this sentence
      
      
      
      
      
 ########clean data##########
 Spelling correct package: 
autocorrect
>>> from autocorrect import spell
>>> spell('HTe')

enchant
>>> import enchant
>>> d = enchant.Dict("en_US")
>>> d.check("Hello")
True
>>> d.check("Helo")
False
>>> d.suggest("Helo")
['He lo', 'He-lo', 'Hello', 'Helot', 'Help', 'Halo', 'Hell', 'Held', 'Helm', 'Hero', "He'll"]

from fuzzywuzzy import fuzz
from fuzzywuzzy import process
fuzz.ratio('fking', 'fucking')

